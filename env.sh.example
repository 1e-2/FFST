# Reproducible training.
export TRAINING_SEED=420420420

# Restart where we left off. Change this to "checkpoint-1234" to start from a specific checkpoint.
export RESUME_CHECKPOINT="latest"

# How often to checkpoint. Depending on your learning rate, you may wish to change this.
# For the default settings with 10 gradient accumulations, more frequent checkpoints might be preferable at first.
export CHECKPOINTING_STEPS=150


export LEARNING_RATE=6e-8 #@param {type:"number"}

# Configure these values.
# Using a Huggingface Hub model:
export MODEL_NAME="stabilityai/stable-diffusion-2-1"
# Using a local path to a huggingface hub model or saved checkpoint:
#export MODEL_NAME="/notebooks/datasets/models/pipeline"

# Use this to append an instance prompt to each caption, used for adding trigger words.
#export INSTANCE_PROMPT="lotr style "

# Location of training data.
export BASE_DIR="/notebooks/datasets"
export INSTANCE_DIR="${BASE_DIR}/training_data"
export OUTPUT_DIR="${BASE_DIR}/models"

# Max number of steps OR epochs can be used. But we default to Epochs.
export MAX_NUM_STEPS=30000
# Will likely overtrain, but that's fine.
export NUM_EPOCHS=25

# Only polynomial is currently supported.
export LR_SCHEDULE="polynomial"
# Whether this is used, depends on whether you have epochs or num_steps in use.
export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))
# Adjust this for your GPU memory size.
export TRAIN_BATCH_SIZE=15

# Leave these alone unless you know what you are doing.
export RESOLUTION=768
export GRADIENT_ACCUMULATION_STEPS=10        # Yes, it slows training down. No, you don't want to change this.
export TEXT_ENCODER_LIMIT=101                # Train the text encoder for % of the process. Buggy.
export TEXT_ENCODER_FREEZE_STRATEGY='before' # before, after, between.
export TEXT_ENCODER_FREEZE_BEFORE=22         # Ignored when using 'after' strategy.
export TEXT_ENCODER_FREEZE_AFTER=24          # Ignored when using 'before' strategy.
export MIXED_PRECISION="bf16"                # Might not be supported on all GPUs. fp32 will be needed for others.
export TRAINING_DYNAMO_BACKEND='no'          # or 'inductor' if you want to brave PyTorch 2 compile issues

# This has to be changed if you're training with multiple GPUs.
export TRAINING_NUM_PROCESSES=1
export TRAINING_NUM_MACHINES=1